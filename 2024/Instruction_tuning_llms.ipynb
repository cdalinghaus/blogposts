{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aee7eb6-6e2c-4bc3-aa18-874d0fdd1720",
   "metadata": {},
   "outputs": [],
   "source": [
    "%META\n",
    "title=Instruction Tuning GPT-2: Fine-Tuning LLMs for Question Answering\n",
    "excerpt=This article explores the process of instruction tuning GPT-2, comparing vanilla performance with finetuned models, and detailing approaches to finetuning using supervised conversational data to improve question-answering capabilities.\n",
    "categories=Machine Learning, NLP, GPT-2, LLM Fine-Tuning, Instruction Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775809a5-3c4f-4ac5-b657-08628e6367ae",
   "metadata": {},
   "source": [
    "With ChatGPT's two-year anniversary around the corner, I had a funny realization: even though I use LLMs pretty much every day, I've never actually tried getting one to follow my own custom instructions. So, I figured, why not give it a go? Plus, I’ve been toying with the idea of fine-tuning an LLM anyway, so now seems like the perfect time to tackle this little gap in my experience! Should be a fun project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aa4c6f-d6b4-4e8b-a87b-75b03d26014a",
   "metadata": {},
   "source": [
    "My computational resources are limited right now so I’ll be using GPT2-medium as a basis model. This will surely not break any records, I’ll be satisfied when the model can give me basic information like the capital of a country. Also, I'll will be basing this on <a href=\"https://github.com/karpathy/nanoGPT\">karpathy/nanoGPT</a>, since it makes finetuning LLMs really easy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beacd02d-2a77-489f-a95a-82440cfa1d2f",
   "metadata": {},
   "source": [
    "Let’s first see how helpful vanilla gpt2-medium is when it comes to giving correct answers to questions:\n",
    "\n",
    "```python sample.py --init_from=gpt2-medium --start=\"What's the capitol of france?\"```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c90e5f7-6957-4905-9387-c534b1bcc0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "What's the capitol of france?\n",
    "\n",
    "Italy, Spain and Portugal have the highest population so far in the world and are in general, the most populous countries in Europe. The country has more than a quarter of the world's population and two-fifths of the total population.\n",
    "—-------------\n",
    "What's the capitol of france?\n",
    "\n",
    "16 U.S.C. § 3509(a)(1) provides that the person convicted of a crime of violence \"shall be guilty of a felony, or shall suffer, upon conviction thereof, upon summary conviction, imprisonment not exceeding fifteen years or a fine not exceeding $50,000 or both.\"\n",
    "—-----------\n",
    " What's the capitol of france?\n",
    "\n",
    "It is not the capital of France. It is the capital of Spain, the capital of Portugal, the capital of Italy, the capital of Belgium, the capital of Luxembourg, the capital of Switzerland, the capital of Germany, the capital of Luxembourg, the capital of Austria, the capital of Belgium, the capital of Luxembourg, and all the capitals of the other major nations, so it is really something like the French capital. So as you can see, it's not their capital."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd24eb38-a0c1-4361-b3fe-0df9cd673277",
   "metadata": {},
   "source": [
    "So… it’s not exactly great at answering questions. With all the buzz around LLMs these days, it’s crazy to think how useless they used to be when they were just spitting out statistically likely sentences purely based on webtext. But we can cheat a bit by tweaking our prompt—just making it clear that the answer to a question is coming up next:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e60589-3a5e-4735-865a-189e79cbf4b0",
   "metadata": {},
   "source": [
    "**```python sample.py --init_from=gpt2-medium --start=\"Question: What's the capitol of france?\\n Answer:\"```**\n",
    "\n",
    "Outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36478f5a-7383-40bf-9e19-0771f3b38a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question: What's the capitol of france?\\n Answer: Le Mans, the capital of France.\n",
    "—\n",
    "Question: What's the capitol of france?\\n Answer: The French capital Uisduy is the third most valuable city in world, and third most populous city. The city is located in the northeast of France, near France's border with Germany, and is the largest city in the region. It is a very vibrant city, with a great number of national parks, museums, and artistic institutions, among others. Its capital is Uisduy, the first city to start being called Uisduy, after its founding in 1689.\n",
    "—\n",
    "(...)\n",
    "-\n",
    "Question: What's the capital of france?\\n Answer: The Capitole de l'État in Paris.\\n\\right now we have the following names on the cover:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e83e97-e3d5-43ca-a24d-ace558c7e09e",
   "metadata": {},
   "source": [
    "Which after a lot of wrong capitals actually mentions Paris(Although the answer is still technically incorrect, but whatever)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768828a4-771e-4d3f-ba06-351fc3fde556",
   "metadata": {},
   "source": [
    "Let’s see how far we can get by sticking to just instruction tuning—so, focusing on fine-tuning with a simple question-answer dataset.  \n",
    "For this, we'll bet using the Question-Answer dataset collected by the <a href=\"https://open-assistant.io\">OpenAssistant</a> project. The actual dataset is <a href=\"https://huggingface.co/datasets/OpenAssistant/oasst2\">OpenAssistant/oasst2</a>\n",
    "\n",
    "We’ll grab the trees jsonl file, which contain the full conversation trees with some annotations and scoring. Really interesting and usefull stuff, honestly. To keep things simple, let’s only consider the prompt from the user (the root of the tree) and the first response that the assistant gives. This simplifies our extraction script significantly. We will just load the trees and dump the conversations to stdout. These conversations will have some very basic XML formatting to indicate beginning and end of user prompts and assistant responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba06db5c-7fc3-4efc-ad4d-b01296dac6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_path = '2023-04-12_oasst_ready.trees.jsonl'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Load the JSON object from the current line\n",
    "        json_object = json.loads(line)\n",
    "\n",
    "        # For every top level reply, print prompt and corresponding reply\n",
    "        for reply in json_object[\"prompt\"][\"replies\"]:\n",
    "            print(\"<conversation>\")\n",
    "            print(\"<user>\", json_object[\"prompt\"][\"text\"], \"</user>\", sep=\"\")\n",
    "            print(\"<assistant>\", reply[\"text\"], \"</assistant>\", sep=\"\")\n",
    "            print(\"</conversation>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca0a60a-9fea-4bd1-baf3-3b476fd41faf",
   "metadata": {},
   "source": [
    "By printing to stdout, we can intuitively verify that our conversational data is at least making some sense. Which it does:\n",
    "\n",
    "**```python export.py > simple_qa_conversations.txt```**  \n",
    "**```head simple_qa_conversations.txt```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5ccdd8-7cf9-4f9e-8e1e-aec546e6ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "<conversation>\n",
    "<user>Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.</user>\n",
    "<assistant>\"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions. (...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e14e76-7c40-4c13-a82a-b936cc34bc37",
   "metadata": {},
   "source": [
    "Now, let's actually run our finetuning. We move the simple_qa_conversations.txt file to data/shakespeare/input.txt to make it use our conversational data file, instead of the default shakespeare texts.  \n",
    "In **```config/finetune_shakespeare.py```**, I've changed _init_from_ to _gpt2-medium_, as I only have 11GB of GPU memory to work with. I also bumped the learning_rate from `3e-5` to `1e-5`, as well as increase max_iters to 200. You can also turn on wandb logging if you want.\n",
    "\n",
    "\n",
    "\n",
    "We then run **```python train.py config/finetune_shakespeare.py```** and wait a few minutes to hours, depending on the hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d92f6f-e31a-44e2-b97f-6810a9b4f3ca",
   "metadata": {},
   "source": [
    "After finetuning has finished (or during, if you switch the device to CPU), we can sample some examples:  \n",
    "**```python sample.py --out_dir=out-shakespeare --compile=False --start=\"<conversation>\\<user>What's the capitol of France?</user>\\n<assistant>\"```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a5a5b8-c49f-4892-ada9-bda9744f3dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "<conversation>                                                                                                                                                                                              \n",
    "<user>What's the capitol of France?</user>                                                                                                                                                                  \n",
    "<assistant>The capital of France is Paris, the capital of France is Paris.                                                                                                                                  \n",
    "</conversation>                                                                                                                                                                                             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2ad873-d8d9-42b0-b910-64d24acd0a72",
   "metadata": {},
   "source": [
    "It works even for smaller things like states of america:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf6a87e-c774-42da-9b9c-9c53b788c594",
   "metadata": {},
   "outputs": [],
   "source": [
    "<conversation>\n",
    "<user>What's the capitol of Texas?</user>\n",
    "<assistant>The capitol of Texas is Austin, TX. The capital is located in the center of the state, between the cities of San Antonio and Austin. However, the city of Austin is not the capital of Texas, as Austin is a city in Texas's 9th congressional district."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90577d49-57cd-40de-84c0-03789ca834c0",
   "metadata": {},
   "source": [
    "It seems to work preddy decently so far. Although there are also a fair share of false answers or wrong formatting, as well as pure nonsense.\n",
    "\n",
    "Performance appears to degrade when you ask about non-western countries, especially when asking for places where the native language is not english:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef06199-b261-470d-80f4-9b5f97adc874",
   "metadata": {},
   "outputs": [],
   "source": [
    "<conversation>\n",
    "<user>What's the capitol of Tripura?</user>\n",
    "<assistant>The capital of Tripura is Jharkhand.</assistant>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33332bbd-4b83-4238-9e76-ea49c5bcb810",
   "metadata": {},
   "source": [
    "It's churning out a lot of cities in India, sadly for me the actual capitol (which would be Agartala) never came up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d142a6c8-cb2e-4570-9f69-601c208d0f24",
   "metadata": {},
   "source": [
    "Finally, let's quickly verify that the fact about the relationship between france and paris is not something that's coming from our finetuning dataset:  \n",
    "**`cat simple_qa_conversations.txt | grep Austin`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90a1f77-a266-45e8-a62b-a66a811cb0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "· \"Austin Powers: International Man of Mystery\" (1997).\n",
    "4. The oldest cat ever, Creme Puff of Austin, TX was born in August of 1967 and passed away in August of 2005, and still holds the Guinness World Record for oldest cat ever at 38 years old."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d007c79-ffe2-45c9-bed5-75d582db186e",
   "metadata": {},
   "source": [
    "While there are plenty of mentions about France, Paris and Texas in the dataset, the only mention of _Austin_ is about Auston Powers, the fictional movie character. It is therefore reasonable to assume that the information about Austin and its relationshop to Texas comes from the pre-trained weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
